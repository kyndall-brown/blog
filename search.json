[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "This is my corner where I will post little things I find about data science and data visualization and share them with you all. I hope you will stick around and check back!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kyndall’s Blog - Homepage",
    "section": "",
    "text": "This is my corner where I will post little things I find about data science and data visualization and share them with you all. I hope you will stick around and check back!"
  },
  {
    "objectID": "posts/generative-art-discussion-and-analysis/index.html",
    "href": "posts/generative-art-discussion-and-analysis/index.html",
    "title": "Generative Art",
    "section": "",
    "text": "The piece is titled “Edmond de Belamy, from La Famille de Belamy,” and was created by Obvious, a French trio consisting of a computer science student and two businessmen. Nobody in Obvious had a background in art… which is obvious. Sorry, I had to go for it.\nThe image shows a blurry impression of a portrait of a white man in a 17th century suit. The black of his suit and hair bleed seamlessly with the splash of black behind his head. The whole thing has the texture of brush strokes, but there is no clear direction to them, only vague swirls. He is positioned at the upper left corner of the canvas, with a generous amount of ivory beige between him and the bottom right corner. In sharp contrast to the man-shape smudge, occupying the lower right corner, is the sharp black text of a formula - the formula the Generative Adversarial Network used to create him.\nThis image is from some time ago, but experts in AI art even at that time were wondering how that image made it to Christie’s. GANs had been used for years by 2015, and other AI artists were already pioneering new methods. The code the team used to create the image was even open source. Christie’s reached out the the trio first, apparently in a bid to gauge the appetite for AI work among the artistic elite. The same place that sold a Leonardo da Vinci for $450 million only set the starting bid for this piece at $7,000.\nI, personally, think it’s just ok. It’s neat, but it doesn’t seem to explore anything else besides rote memorization and repetition, like a child in school. The white space at the bottom is like a metaphor for how much more the image would need to say something new in a space that is supposed to be revolutionary. But those are just my thoughts!"
  },
  {
    "objectID": "posts/charts-dos-and-donts/index.html",
    "href": "posts/charts-dos-and-donts/index.html",
    "title": "Charts: Do’s and Don’ts",
    "section": "",
    "text": "First up is this chart:\n\nData visualization is my passion.\nThe cool thing about data visualization is that even a layperson can tell which charts are good. Being intuitive to a wide range of people is what makes it good. So what we see here is an example that everyone, literally everyone, can tell leaves a lot to be desired.\nFirst of all, the color scheme looks like a rave party. My first intuition is that the columns of a similar color are the same country, but the chart actually shows the colors are for years. If the colors represented countries, then the chart would be more readable because it would be easier to see the rises and falls of each country at a glance. Also, the 3D angle of the chart makes it hard to compare the bars in the middle to the axes. Hoe many bananas did Ecuador export in 1994? That may be (relatively) easy to answer. But how many bananas did Guatemala export in 2000? I have no idea. It would be faster to Google it than stare at this chart to try and find out.\nAnd the bananas don’t help.\nIf you have to squint at a chart, pore over it endlessly, then it is a bad chart. The point of charts is to help humans understand and contextualize raw numbers that otherwise would not be very understandable. People can catch on quickly and find new relationships between things when they see them in a well made, intuitive chart."
  },
  {
    "objectID": "posts/murrells-rgraphics-basic-r-programs/index.html",
    "href": "posts/murrells-rgraphics-basic-r-programs/index.html",
    "title": "Paul Murrell’s RGraphics",
    "section": "",
    "text": "This graph depicts the Happy Planet Index against life expectancy for each country:\n\nThis histogram depicts the ecological footprint of all countries:\n\nThis graph depicts a boxplot of nations’ Ladder of Life - Wellbeing score by continent:\n\nThis is a perspective plot:\n\nThe final graph is a pie chart showing the global population by continent:\n\nThe code used to produce these graphs is found below:\n# plotting exercise using happy planet index data:\nsetwd(\"C:/Users/professionalClassic/Desktop/_fall2022/EPPS_6356/assign_02\")\nhpi <- read.csv(\"happyplanetindex.csv\")\n\n#renaming and adding data to our dataframe\nhpi$Continent_num <- hpi$Continent\nhpi[\"Continent\"][hpi[\"Continent\"] == 1] <- \"Latin Amer.\"\nhpi[\"Continent\"][hpi[\"Continent\"] == 2] <- \"N. Amer./Oc.\"\nhpi[\"Continent\"][hpi[\"Continent\"] == 3] <- \"W. Europe\"\nhpi[\"Continent\"][hpi[\"Continent\"] == 4] <- \"Mid. East\"\nhpi[\"Continent\"][hpi[\"Continent\"] == 5] <- \"Sub-S. Africa\"\nhpi[\"Continent\"][hpi[\"Continent\"] == 6] <- \"S. Asia\"\nhpi[\"Continent\"][hpi[\"Continent\"] == 7] <- \"E. Europe/C. Asia\"\nhpi[\"Continent\"][hpi[\"Continent\"] == 8] <- \"E. Asia\"\nhpi$Hemisphere <- hpi$Continent\nhpi[\"Hemisphere\"][hpi[\"Hemisphere\"] == \"Latin Amer.\"] <- \"South\"\nhpi[\"Hemisphere\"][hpi[\"Hemisphere\"] == \"N. Amer./Oc.\"] <- \"North\"\nhpi[\"Hemisphere\"][hpi[\"Hemisphere\"] == \"W. Europe\"] <- \"North\"\nhpi[\"Hemisphere\"][hpi[\"Hemisphere\"] == \"Mid. East\"] <- \"North\"\nhpi[\"Hemisphere\"][hpi[\"Hemisphere\"] == \"Sub-S. Africa\"] <- \"South\"\nhpi[\"Hemisphere\"][hpi[\"Hemisphere\"] == \"S. Asia\"] <- \"North\"\nhpi[\"Hemisphere\"][hpi[\"Hemisphere\"] == \"E. Europe/C. Asia\"] <- \"North\"\nhpi[\"Hemisphere\"][hpi[\"Hemisphere\"] == \"E. Asia\"] <- \"North\"\n\n\n# line graph:\npar(mar=c(4, 4, 5, 4)) \nplot.new()\nplot.window(xlim=c(0, 160), ylim=c(50,90))\nlines(hpi$HPI_rank, hpi$Life.Expectancy_years, col=\"steelblue4\")\npoints(hpi$HPI_rank, hpi$Life.Expectancy_years, pch=5, col=\"coral3\", cex=0.5)\npar(col=\"cornsilk3\", fg=\"cornsilk3\", col.axis=\"cornsilk3\")\naxis(1, at=seq(0, 160, 20)) # What is the first number standing for?\naxis(2, at=seq(50, 90, 10))\naxis(4, at=seq(50, 90, 10))\nbox(bty=\"u\")\npar(col=\"navajowhite4\")\nmtext(\"Happy Planet Index Ranking\", side=1, line=2, cex=0.8)\nmtext(\"Life Expectancy\", side=2, line=2, las=0, cex=0.8)\nmtext(\"HPI Ranking vs. Life Expectancy\", side=3, line=2, cex=2, col=\"firebrick4\")\ntext(x=40, y=55, \"Life expectancy varies greatly\", cex=0.8)\n\n# Histogram\npar(mar=c(4, 4, 3, 4), fg=\"aquamarine4\", cex=0.8,\n    col.axis=\"aquamarine4\", bg=\"white\", col.lab=\"plum3\", col.main=\"plum3\"\n    )\nhist(hpi$Ecological_Footprint_gha, ylim=c(0, 70),freq=TRUE, density=20, \n     angle=70, col=\"darkseagreen2\",\n     border=\"aquamarine4\", main=\"Ecological Footprint\", xlab=\"G ha of resource useage\",\n     labels=TRUE)\nbox(bty=\"u\")\ntext(11, 60, \"78% of countries use under 5 g ha of resources\")\n\n#boxplot\npar(mar=c(3, 4.1, 2.5, 4), fg=\"cornflowerblue\", col.axis=\"navyblue\")\nboxplot(Ladder_of_life_Wellbeing ~ Continent_num, data = hpi,\n        xlab=\"\", col=c(\"darksalmon\", \"lightskyblue\", \"lightskyblue\",\n                       \"lightskyblue\", \"darksalmon\", \"lightskyblue\",\n                       \"lightskyblue\", \"lightskyblue\"),\n        ylab=\"\", ylim=c(2,8))\nmtext(\"Continents\", side=1, line=2, cex=0.8, col=\"steelblue4\")\nmtext(\"Ladder of Life Wellbeing\", side=2, line=2, col=\"steelblue4\")\nmtext(\"Ladder of Life by Continent\", side=3, line=.5, cex=2,\n      col=\"steelblue4\")\ntext(1, 7.5, \"L. Amer.\")\ntext(2, 7.8, \"N. Amer./Oc.\")\ntext(3, 5.5, \"W. Eur.\")\ntext(4, 3.5, \"Mid. East\")\ntext(5, 6.8, \"Sub. Africa\")\ntext(6, 5.7, \"S. Asia\")\ntext(7, 7.2, \"E. Eur.\")\ntext(8, 7.2, \"E. Asia\")\nlegend(0.2, 2.9, c(\"Northern Hemisphere\", \"Southern Hemisphere\"), \n       fill = c(\"lightskyblue\", \"darksalmon\"), \n       bty=\"n\")\n\n# Persp (source: https://github.com/datageneration/datavisualization/blob/master/R/murrell01.R)\nx <- seq(-10, 10, length= 40)\ny <- x\nf <- function(x,y) { r <- sqrt(x^2+y^2); 10 * tan(r)/(0.2*r)}\nz <- outer(x, y, f)\nz[is.na(z)] <- 1\n# 0.5 to include z axis label\npar(mar=c(0, 0.5, 0, 0), lwd=0.5)\npersp(x, y, z, theta = 60, phi = 10, \n      expand = 0.5)\n\n# Pie Chart\npar(mar=c(0, 2, 1.5, 2), xpd=TRUE, cex=0.9)\ncont.pop <- c(with(hpi, sum(Population_thousands[Continent_num == 1])), \n              with(hpi, sum(Population_thousands[Continent_num == 2])),\n              with(hpi, sum(Population_thousands[Continent_num == 3])),\n              with(hpi, sum(Population_thousands[Continent_num == 4])),\n              with(hpi, sum(Population_thousands[Continent_num == 5])),\n              with(hpi, sum(Population_thousands[Continent_num == 6])),\n              with(hpi, sum(Population_thousands[Continent_num == 7])),\n              with(hpi, sum(Population_thousands[Continent_num == 8])))\nnames(cont.pop) <- c(\"Latin America\", \"North America and Oceana\",\n                      \"Western Europe\", \"Middle East and North Africa\",\n                     \"Sub-Saharan Africa\", \"South Asia\",\n                     \"Eastern Europe and Central Asia\", \"East Asia\")\npie(cont.pop, col = c(\"lightcoral\",\"aquamarine\", \"cyan2\", \"lightseagreen\",\n                      \"orangered1\",\"cornflowerblue\", \"royalblue1\",\n                      \"slategray2\")) \nmtext(\"Global Population by Continent\", cex = 1.5, )\nThank you for visiting!"
  },
  {
    "objectID": "posts/generative-leaves/index.html",
    "href": "posts/generative-leaves/index.html",
    "title": "Generative Leaves",
    "section": "",
    "text": "I created a few more leaves by changing the code. For my next leaf, I changed the color to sienna2 and altered the code like this:\n{angle=20} depth=6\nThe resulting leaf looked like this:\n\nIt’s interesting how the rest of the leaf was unchanged by it added this extra leaf on the side, I didn’t expect that!\nI made one final leaf by changing the color to lightcyan and altering the code like this:\n{axiom=“X”} rules=list(“X”=“F-[[X]+X]+F[+FX]-X”, “F”=“FF+1”) angle=23 depth=7\nI changed the angle to 23, but also the depth to 7 and added +1 to “F” in the axiom variable. The result was shocking:\n\nIt’s amazing how a simple change can produce such large effects!"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Kyndall’s Blog - Homepage",
    "section": "",
    "text": "Basic Graphics for a Basic Gal\n\n\n\n\nanalysis\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2022\n\n\nKyndall Brown\n\n\n\n\n\n\n  \n\n\n\n\n\nBut mostly don’ts\n\n\n\n\nanalysis\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2022\n\n\nKyndall Brown\n\n\n\n\n\n\n  \n\n\n\n\n\nAnalysis and discussion\n\n\n\n\nnews\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2022\n\n\nKyndall Brown\n\n\n\n\n\n\n  \n\n\n\n\n\nClassic graphing fun!\n\n\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2022\n\n\nKyndall Brown\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Kyndall’s Blog - All Posts",
    "section": "",
    "text": "A reactive website using Shiny\n\n\n\n\nvisualization\n\n\nEPPS6356\n\n\nassignments\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\nKyndall Brown\n\n\n\n\n\n\n  \n\n\n\n\n\nCharts 3-5\n\n\n\n\nvisualization\n\n\nEPPS6356\n\n\nassignments\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\nKyndall Brown\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvisualization\n\n\nEPPS6356\n\n\nprepare for class\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2022\n\n\nKyndall Brown\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvisualization\n\n\nEPPS6356\n\n\nprepare for class\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2022\n\n\nKyndall Brown\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Knuth (1984)\n\n\n\n\nprogramming\n\n\nEPPS6356\n\n\nprepare for class\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2022\n\n\nKyndall Brown\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvisualization\n\n\nEPPS6356\n\n\nprepare for class\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2022\n\n\nKyndall Brown\n\n\n\n\n\n\n  \n\n\n\n\n\nDiscussion of new technologies\n\n\n\n\nvisualization\n\n\nEPPS6356\n\n\nprepare for class\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2022\n\n\nKyndall Brown\n\n\n\n\n\n\n  \n\n\n\n\n\nBig data analysis and its secret pitfalls\n\n\n\n\nEPPS6356\n\n\nPrepare for Class\n\n\ndiscussion\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2022\n\n\nKyndall Brown\n\n\n\n\n\n\n  \n\n\n\n\n\nComparing regression models and using base R functions.\n\n\n\n\nvisualization\n\n\nEPPS6356\n\n\nassignments\n\n\n\n\n\n\n\n\n\n\n\nOct 11, 2022\n\n\nKyndall Brown\n\n\n\n\n\n\n  \n\n\n\n\n\nBasic Graphics for a Basic Gal\n\n\n\n\nanalysis\n\n\nvisualization\n\n\nEPPS6356\n\n\nassignments\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\nKyndall Brown\n\n\n\n\n\n\n  \n\n\n\n\n\nVariable width bar chart and stacked bar graphs\n\n\n\n\nvisualization\n\n\nEPPS6356\n\n\nhackathons\n\n\nassignments\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2022\n\n\nKyndall Brown\n\n\n\n\n\n\n  \n\n\n\n\n\nBut mostly don’ts\n\n\n\n\nanalysis\n\n\nvisualization\n\n\nEPPS6356\n\n\nassignments\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2022\n\n\nKyndall Brown\n\n\n\n\n\n\n  \n\n\n\n\n\nAnalysis and discussion\n\n\n\n\nnews\n\n\nanalysis\n\n\nEPPS6356\n\n\nassignments\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2022\n\n\nKyndall Brown\n\n\n\n\n\n\n  \n\n\n\n\n\nClassic graphing fun!\n\n\n\n\nvisualization\n\n\nEPPS6356\n\n\nassignments\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2022\n\n\nKyndall Brown\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/first-hackathon-results/index.html",
    "href": "posts/first-hackathon-results/index.html",
    "title": "First Hackathon Results",
    "section": "",
    "text": "Chart #1: Variable Width Bar Chart\n\n\n\nChart #2: Stacked Bar Charts\n\nThanks to my team: Leslie, Ana, Theophilus, Michelle and myself. Shoutout to Michelle for staying up late and working hard, and to Leslie for making the final breakthrough that led to our finished charts!"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Second Hackathon Results\n\n\nA reactive website using Shiny\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n\n\nMore Great Charts!\n\n\nCharts 3-5\n\n\n\n\n\n\nOct 26, 2022\n\n\n\n\n\n\n\n\nComparing Different Styles of Data Visualization\n\n\n\n\n\n\n\n\n\nOct 18, 2022\n\n\n\n\n\n\n\n\nReviewing the Work of Thomas Lind Pederson\n\n\n\n\n\n\n\n\n\nOct 18, 2022\n\n\n\n\n\n\n\n\nAn Overview of Literate Programming\n\n\nFrom Knuth (1984)\n\n\n\n\n\n\nOct 18, 2022\n\n\n\n\n\n\n\n\nThe Elements of Music Visualization\n\n\n\n\n\n\n\n\n\nOct 18, 2022\n\n\n\n\n\n\n\n\nData Visualization and Data Science (EMBL)\n\n\nDiscussion of new technologies\n\n\n\n\n\n\nOct 16, 2022\n\n\n\n\n\n\n\n\nThe Parable of Google Flu\n\n\nBig data analysis and its secret pitfalls\n\n\n\n\n\n\nOct 16, 2022\n\n\n\n\n\n\n\n\nAnscombe’s Quartet\n\n\nComparing regression models and using base R functions.\n\n\n\n\n\n\nOct 11, 2022\n\n\n\n\n\n\n\n\nPaul Murrell’s RGraphics\n\n\nBasic Graphics for a Basic Gal\n\n\n\n\n\n\nOct 5, 2022\n\n\n\n\n\n\n\n\nFirst Hackathon Results\n\n\nVariable width bar chart and stacked bar graphs\n\n\n\n\n\n\nOct 3, 2022\n\n\n\n\n\n\n\n\nCharts: Do’s and Don’ts\n\n\nBut mostly don’ts\n\n\n\n\n\n\nSep 30, 2022\n\n\n\n\n\n\n\n\nGenerative Art\n\n\nAnalysis and discussion\n\n\n\n\n\n\nSep 30, 2022\n\n\n\n\n\n\n\n\nGenerative Leaves\n\n\nClassic graphing fun!\n\n\n\n\n\n\nSep 28, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/comparing-regression-models/index.html",
    "href": "posts/comparing-regression-models/index.html",
    "title": "Comparing Regression Models",
    "section": "",
    "text": "Not up quite yet, I’m working on it though!"
  },
  {
    "objectID": "posts/epps-6356-assignment-01/epps-6356-assignment-01.html",
    "href": "posts/epps-6356-assignment-01/epps-6356-assignment-01.html",
    "title": "Series: EPPS 6356 Assignment #1",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/epps-6356-assignment-01/generative-art-discussion-and-analysis/index.html",
    "href": "posts/epps-6356-assignment-01/generative-art-discussion-and-analysis/index.html",
    "title": "Generative Art",
    "section": "",
    "text": "The piece is titled “Edmond de Belamy, from La Famille de Belamy,” and was created by Obvious, a French trio consisting of a computer science student and two businessmen. Nobody in Obvious had a background in art… which is obvious. Sorry, I had to go for it.\nThe image shows a blurry impression of a portrait of a white man in a 17th century suit. The black of his suit and hair bleed seamlessly with the splash of black behind his head. The whole thing has the texture of brush strokes, but there is no clear direction to them, only vague swirls. He is positioned at the upper left corner of the canvas, with a generous amount of ivory beige between him and the bottom right corner. In sharp contrast to the man-shape smudge, occupying the lower right corner, is the sharp black text of a formula - the formula the Generative Adversarial Network used to create him.\nThis image is from some time ago, but experts in AI art even at that time were wondering how that image made it to Christie’s. GANs had been used for years by 2015, and other AI artists were already pioneering new methods. The code the team used to create the image was even open source. Christie’s reached out the the trio first, apparently in a bid to gauge the appetite for AI work among the artistic elite. The same place that sold a Leonardo da Vinci for $450 million only set the starting bid for this piece at $7,000.\nI, personally, think it’s just ok. It’s neat, but it doesn’t seem to explore anything else besides rote memorization and repetition, like a child in school. The white space at the bottom is like a metaphor for how much more the image would need to say something new in a space that is supposed to be revolutionary. But those are just my thoughts!"
  },
  {
    "objectID": "posts/epps-6356-assignment-01/generative-leaves/index.html",
    "href": "posts/epps-6356-assignment-01/generative-leaves/index.html",
    "title": "Generative Leaves",
    "section": "",
    "text": "I created a few more leaves by changing the code. For my next leaf, I changed the color to sienna2 and altered the code like this:\n{angle=20} depth=6\nThe resulting leaf looked like this:\n\nIt’s interesting how the rest of the leaf was unchanged by it added this extra leaf on the side, I didn’t expect that!\nI made one final leaf by changing the color to lightcyan and altering the code like this:\n{axiom=“X”} rules=list(“X”=“F-[[X]+X]+F[+FX]-X”, “F”=“FF+1”) angle=23 depth=7\nI changed the angle to 23, but also the depth to 7 and added +1 to “F” in the axiom variable. The result was shocking:\n\nIt’s amazing how a simple change can produce such large effects!"
  },
  {
    "objectID": "posts/epps-6356-assignment-01/index.html",
    "href": "posts/epps-6356-assignment-01/index.html",
    "title": "Series: EPPS 6356 Assignment #1",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/epps6356-assignment01/index.html",
    "href": "posts/epps6356-assignment01/index.html",
    "title": "Series: EPPS 6356 Assignment #1",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/epps6356-assignment01/generative-art-discussion-and-analysis/index.html",
    "href": "posts/epps6356-assignment01/generative-art-discussion-and-analysis/index.html",
    "title": "Generative Art",
    "section": "",
    "text": "The piece is titled “Edmond de Belamy, from La Famille de Belamy,” and was created by Obvious, a French trio consisting of a computer science student and two businessmen. Nobody in Obvious had a background in art… which is obvious. Sorry, I had to go for it.\nThe image shows a blurry impression of a portrait of a white man in a 17th century suit. The black of his suit and hair bleed seamlessly with the splash of black behind his head. The whole thing has the texture of brush strokes, but there is no clear direction to them, only vague swirls. He is positioned at the upper left corner of the canvas, with a generous amount of ivory beige between him and the bottom right corner. In sharp contrast to the man-shape smudge, occupying the lower right corner, is the sharp black text of a formula - the formula the Generative Adversarial Network used to create him.\nThis image is from some time ago, but experts in AI art even at that time were wondering how that image made it to Christie’s. GANs had been used for years by 2015, and other AI artists were already pioneering new methods. The code the team used to create the image was even open source. Christie’s reached out the the trio first, apparently in a bid to gauge the appetite for AI work among the artistic elite. The same place that sold a Leonardo da Vinci for $450 million only set the starting bid for this piece at $7,000.\nI, personally, think it’s just ok. It’s neat, but it doesn’t seem to explore anything else besides rote memorization and repetition, like a child in school. The white space at the bottom is like a metaphor for how much more the image would need to say something new in a space that is supposed to be revolutionary. But those are just my thoughts!"
  },
  {
    "objectID": "posts/epps6356-assignment01/generative-leaves/index.html",
    "href": "posts/epps6356-assignment01/generative-leaves/index.html",
    "title": "Generative Leaves",
    "section": "",
    "text": "# Title Fall color\n# Credit: https://fronkonstin.com\n\n# Install packages\n\ninstall.packages(\"gsubfn\")\ninstall.packages(\"tidyverse\")\nlibrary(gsubfn)\nlibrary(tidyverse)\n\n# Define elements in plant art\n# Each image corresponds to a different axiom, rules, angle and depth\n\n# Leaf of Fall\n\naxiom=\"X\"\nrules=list(\"X\"=\"F-[[X]+X]+F[+FX]-X\", \"F\"=\"FF\")\nangle=22.5\ndepth=6\n\n\nfor (i in 1:depth) axiom=gsubfn(\".\", rules, axiom)\n\nactions=str_extract_all(axiom, \"\\\\d*\\\\+|\\\\d*\\\\-|F|L|R|\\\\[|\\\\]|\\\\|\") %>% unlist\n\nstatus=data.frame(x=numeric(0), y=numeric(0), alfa=numeric(0))\npoints=data.frame(x1 = 0, y1 = 0, x2 = NA, y2 = NA, alfa=90, depth=1)\n\n\n# Generating data\n# Note: may take a minute or two\n\nfor (action in actions)\n{\n  if (action==\"F\")\n  {\n    x=points[1, \"x1\"]+cos(points[1, \"alfa\"]*(pi/180))\n    y=points[1, \"y1\"]+sin(points[1, \"alfa\"]*(pi/180))\n    points[1,\"x2\"]=x\n    points[1,\"y2\"]=y\n    data.frame(x1 = x, y1 = y, x2 = NA, y2 = NA,\n               alfa=points[1, \"alfa\"],\n               depth=points[1,\"depth\"]) %>% rbind(points)->points\n  }\n  if (action %in% c(\"+\", \"-\")){\n    alfa=points[1, \"alfa\"]\n    points[1, \"alfa\"]=eval(parse(text=paste0(\"alfa\",action, angle)))\n  }\n  if(action==\"[\"){\n    data.frame(x=points[1, \"x1\"], y=points[1, \"y1\"], alfa=points[1, \"alfa\"]) %>%\n      rbind(status) -> status\n    points[1, \"depth\"]=points[1, \"depth\"]+1\n  }\n\n  if(action==\"]\"){\n    depth=points[1, \"depth\"]\n    points[-1,]->points\n    data.frame(x1=status[1, \"x\"], y1=status[1, \"y\"], x2=NA, y2=NA,\n               alfa=status[1, \"alfa\"],\n               depth=depth-1) %>%\n      rbind(points) -> points\n    status[-1,]->status\n  }\n}\n\nggplot() +\n  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2),\n               lineend = \"round\",\n               color=\"burlywood3\", # Set your own Fall color?\n               data=na.omit(points)) +\n  coord_fixed(ratio = 1) +\n  theme_void() # No grid nor axes\nAfter running the code, it created this leaf:\n\nI created a few more leaves by changing the code. For my next leaf, I changed the color to sienna2 and altered the code like this:\nangle=20\ndepth=6\nThe resulting leaf looked like this:\n\nIt’s interesting how the rest of the leaf was unchanged by it added this extra leaf on the side, I didn’t expect that!\nI made one final leaf by changing the color to lightcyan and altering the code like this:\naxiom=\"X\"\nrules=list(\"X\"=\"F-\\[\\[X\\]+X\\]+F\\[+FX\\]-X\", \"F\"=\"FF+1\")\nangle=23\ndepth=7\nI changed the angle to 23, but also the depth to 7 and added +1 to “F” in the axiom variable. The result was shocking:\n\nIt’s amazing how a simple change can produce such large effects!"
  },
  {
    "objectID": "posts/epps6356-assignment01/series-epps6356-assignment01.html",
    "href": "posts/epps6356-assignment01/series-epps6356-assignment01.html",
    "title": "Series: EPPS 6356 Assignment #1",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/epps6356-assignment01.html",
    "href": "posts/epps6356-assignment01.html",
    "title": "Series: EPPS 6356 Assignment #1",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "epps6356-assignment01.html",
    "href": "epps6356-assignment01.html",
    "title": "Series: EPPS 6356 Assignment #1",
    "section": "",
    "text": "Sep 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/anscombes-quartet/index.html",
    "href": "posts/anscombes-quartet/index.html",
    "title": "Anscombe’s Quartet",
    "section": "",
    "text": "Today, I will be looking at the Anscombe’s quartet and using the base R functions to modify the outputs of the plot() function.\nTo give a brief overview of Anscombe’s quartet, according to Wikipedia:\n\nAnscombe’s quartet comprises four data sets that have nearly identical simple descriptive statistics, yet have very different distributions and appear very different when graphed… They were constructed in 1973 by the statistician Francis Anscombe to demonstrate both the importance of graphing data when analyzing it, and the effect of outliers and other influential observations on statistical properties.\n\nAll four datasets have the same means of X and Y, sample variances of X and Y, correlation between X and Y, regression output, and \\(R^2\\).\nThe code I used to produce the first set of graphs came from here, as part of my assignment.\nNow I have said enough, let’s get to graphing!\nThe following graphs are the standard ones that R produces when using the plot() function on the four different data series:\n\n\n\n\n\nThis is pretty boring.\nBut thankfully, I can use R to put them all in the same graph and spruce them up and give each dataset a better title! This is what I came up with:\n\nIt may seem cheesy, but I am even proud I managed to figure out how to make a multicolored title like that. Hooray!\nBut I can do even better. Instead of using the base-r, we can use ggplot2!\nThis is my final plot using theme_solarized:\n\nI really like how it turned out! The code is below.\n\n# Anscombe (1973) Quartlet\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nlibrary(ggthemes)\n\ndata(anscombe)  # Load Anscombe's data\nView(anscombe) # View the data\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n# Create four model objects\nlm1 <- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm2 <- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm3 <- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm4 <- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nplot(anscombe$x1,anscombe$y1)\nabline(coefficients(lm1))\n\n\n\nplot(anscombe$x2,anscombe$y2)\nabline(coefficients(lm2))\n\n\n\nplot(anscombe$x3,anscombe$y3)\nabline(coefficients(lm3))\n\n\n\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\n\n\n\n## Fancy version (per help file)\n\nff <- y ~ x\n# concatenates lm + 1 - 4 and sets them as names for columns 1 - 4\nmods <- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {  # opens the for loop, i represents 1,2,3, and 4\n  ## lapply pastes y then x followed by 1 - 4\n  ff[2:3] <- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] <- as.name(paste0(\"y\", i))\n  ##      ff[[3]] <- as.name(paste0(\"x\", i))\n  ## assigns each regression to mods 1- 4. lmi allows it to be printed\n  mods[[i]] <- lmi <- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(>F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(>F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(>F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(>F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsapply(mods, coef)  # applies a function over a list and returns the result\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\n    ## but only used x1\nlapply(mods, function(fm) coef(summary(fm))) #lapply will use the whole list\n\n$lm1\n             Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(>|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\n# Preparing for the plots\npar(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0.1, 0.1, 2, 0.1), \n    family = 'serif')\nbgcol = c(\"deepskyblue\", \"paleturquoise3\", \"orchid1\", \"coral\")\nfgcol = c(\"mediumorchid4\", \"seagreen\", \"purple4\", \"orangered\")\nabcol = c(\"red\", \"red\", \"blue\", \"blue\")\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] <- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, pch = i + 20, bg = bgcol[i], \n       cex = 1.2, fg = fgcol[i],\n       xlim = c(3, 19), ylim = c(3, 13), xlab = \"\", ylab = \"\")\n  abline(mods[[i]], col = abcol[i])\n  mtext(\"X\", side = 1, line = 2, col = fgcol[i])\n  mtext(\"Y\", side = 2, line = 2, col = fgcol[i])\n  mtext(paste0(\"Dataset \", i), side = 3, line = 0, col = fgcol[i])\n}\n\nttltxt = c(\"Anscombe's\", \"Four\", \"Regression\", \"Datasets\")\nfor(i in 1:4) {\n  mtext(ttltxt[i], outer = TRUE, cex = 1.5, family = \"serif\", col = fgcol[i],\n        adj = 0 + i * 0.21)\n}\n\n\n\npt1 <- ggplot(anscombe, aes(x=x1, y=y1)) +\n  geom_point(color = \"deepskyblue\") + theme_solarized() +\n  ggtitle(\"Dataset 1\")\n\npt2 <- ggplot(anscombe, aes(x=x2, y=y2)) +\n  geom_point(color = \"paleturquoise3\") + theme_solarized() +\n  ggtitle(\"Dataset 2\")\n\npt3 <- ggplot(anscombe, aes(x=x3, y=y3)) + theme_solarized() +\n  geom_point(color = \"orchid1\") +\n  ggtitle(\"Dataset 3\")\n\npt4 <- ggplot(anscombe, aes(x=x4, y=y4)) + theme_solarized() +\n  geom_point(color = \"coral\") +\n  ggtitle(\"Dataset 4\")\n\ngridExtra::grid.arrange(pt1, pt2, pt3, pt4, ncol = 2)"
  },
  {
    "objectID": "posts/anscombes-quartet/index.html#simple-version",
    "href": "posts/anscombes-quartet/index.html#simple-version",
    "title": "Anscombe’s Quartet",
    "section": "Simple version",
    "text": "Simple version\nplot(anscombe\\(x1,anscombe\\)y1) summary(anscombe)"
  },
  {
    "objectID": "posts/anscombes-quartet/index.html#fancy-version-per-help-file",
    "href": "posts/anscombes-quartet/index.html#fancy-version-per-help-file",
    "title": "Anscombe’s Quartet",
    "section": "Fancy version (per help file)",
    "text": "Fancy version (per help file)\nff <- y ~ x # concatenates lm + 1 - 4 and sets them as names for columns 1 - 4 mods <- setNames(as.list(1:4), paste0(“lm”, 1:4))"
  },
  {
    "objectID": "epps-6356-assignment-1.html",
    "href": "epps-6356-assignment-1.html",
    "title": "Series: EPPS 6356 Assignment #1",
    "section": "",
    "text": "Sep 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "epps-6356-prepare-for-class-5.html",
    "href": "epps-6356-prepare-for-class-5.html",
    "title": "Series: EPPS 6356 Prepare For Class #5",
    "section": "",
    "text": "Oct 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "epps-6356-prepare-for-class-6.html",
    "href": "epps-6356-prepare-for-class-6.html",
    "title": "Series: EPPS 6356 Prepare For Class #6",
    "section": "",
    "text": "Oct 18, 2022\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\nOct 18, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "epps-6356-prepare-for-class-7.html",
    "href": "epps-6356-prepare-for-class-7.html",
    "title": "Series: EPPS 6356 Prepare For Class #7",
    "section": "",
    "text": "Oct 18, 2022\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\nOct 18, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/epps-6356-assignment-1/generative-art-discussion-and-analysis/index.html",
    "href": "posts/epps-6356-assignment-1/generative-art-discussion-and-analysis/index.html",
    "title": "Generative Art",
    "section": "",
    "text": "The piece is titled “Edmond de Belamy, from La Famille de Belamy,” and was created by Obvious, a French trio consisting of a computer science student and two businessmen. Nobody in Obvious had a background in art… which is obvious. Sorry, I had to go for it.\nThe image shows a blurry impression of a portrait of a white man in a 17th century suit. The black of his suit and hair bleed seamlessly with the splash of black behind his head. The whole thing has the texture of brush strokes, but there is no clear direction to them, only vague swirls. He is positioned at the upper left corner of the canvas, with a generous amount of ivory beige between him and the bottom right corner. In sharp contrast to the man-shape smudge, occupying the lower right corner, is the sharp black text of a formula - the formula the Generative Adversarial Network used to create him.\nThis image is from some time ago, but experts in AI art even at that time were wondering how that image made it to Christie’s. GANs had been used for years by 2015, and other AI artists were already pioneering new methods. The code the team used to create the image was even open source. Christie’s reached out the the trio first, apparently in a bid to gauge the appetite for AI work among the artistic elite. The same place that sold a Leonardo da Vinci for $450 million only set the starting bid for this piece at $7,000.\nI, personally, think it’s just ok. It’s neat, but it doesn’t seem to explore anything else besides rote memorization and repetition, like a child in school. The white space at the bottom is like a metaphor for how much more the image would need to say something new in a space that is supposed to be revolutionary. But those are just my thoughts!"
  },
  {
    "objectID": "posts/epps-6356-assignment-1/generative-leaves/index.html",
    "href": "posts/epps-6356-assignment-1/generative-leaves/index.html",
    "title": "Generative Leaves",
    "section": "",
    "text": "# Title Fall color\n# Credit: https://fronkonstin.com\n\n# Install packages\n\ninstall.packages(\"gsubfn\")\ninstall.packages(\"tidyverse\")\nlibrary(gsubfn)\nlibrary(tidyverse)\n\n# Define elements in plant art\n# Each image corresponds to a different axiom, rules, angle and depth\n\n# Leaf of Fall\n\naxiom=\"X\"\nrules=list(\"X\"=\"F-[[X]+X]+F[+FX]-X\", \"F\"=\"FF\")\nangle=22.5\ndepth=6\n\n\nfor (i in 1:depth) axiom=gsubfn(\".\", rules, axiom)\n\nactions=str_extract_all(axiom, \"\\\\d*\\\\+|\\\\d*\\\\-|F|L|R|\\\\[|\\\\]|\\\\|\") %>% unlist\n\nstatus=data.frame(x=numeric(0), y=numeric(0), alfa=numeric(0))\npoints=data.frame(x1 = 0, y1 = 0, x2 = NA, y2 = NA, alfa=90, depth=1)\n\n\n# Generating data\n# Note: may take a minute or two\n\nfor (action in actions)\n{\n  if (action==\"F\")\n  {\n    x=points[1, \"x1\"]+cos(points[1, \"alfa\"]*(pi/180))\n    y=points[1, \"y1\"]+sin(points[1, \"alfa\"]*(pi/180))\n    points[1,\"x2\"]=x\n    points[1,\"y2\"]=y\n    data.frame(x1 = x, y1 = y, x2 = NA, y2 = NA,\n               alfa=points[1, \"alfa\"],\n               depth=points[1,\"depth\"]) %>% rbind(points)->points\n  }\n  if (action %in% c(\"+\", \"-\")){\n    alfa=points[1, \"alfa\"]\n    points[1, \"alfa\"]=eval(parse(text=paste0(\"alfa\",action, angle)))\n  }\n  if(action==\"[\"){\n    data.frame(x=points[1, \"x1\"], y=points[1, \"y1\"], alfa=points[1, \"alfa\"]) %>%\n      rbind(status) -> status\n    points[1, \"depth\"]=points[1, \"depth\"]+1\n  }\n\n  if(action==\"]\"){\n    depth=points[1, \"depth\"]\n    points[-1,]->points\n    data.frame(x1=status[1, \"x\"], y1=status[1, \"y\"], x2=NA, y2=NA,\n               alfa=status[1, \"alfa\"],\n               depth=depth-1) %>%\n      rbind(points) -> points\n    status[-1,]->status\n  }\n}\n\nggplot() +\n  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2),\n               lineend = \"round\",\n               color=\"burlywood3\", # Set your own Fall color?\n               data=na.omit(points)) +\n  coord_fixed(ratio = 1) +\n  theme_void() # No grid nor axes\nAfter running the code, it created this leaf:\n\nI created a few more leaves by changing the code. For my next leaf, I changed the color to sienna2 and altered the code like this:\nangle=20\ndepth=6\nThe resulting leaf looked like this:\n\nIt’s interesting how the rest of the leaf was unchanged by it added this extra leaf on the side, I didn’t expect that!\nI made one final leaf by changing the color to lightcyan and altering the code like this:\naxiom=\"X\"\nrules=list(\"X\"=\"F-\\[\\[X\\]+X\\]+F\\[+FX\\]-X\", \"F\"=\"FF+1\")\nangle=23\ndepth=7\nI changed the angle to 23, but also the depth to 7 and added +1 to “F” in the axiom variable. The result was shocking:\n\nIt’s amazing how a simple change can produce such large effects!"
  },
  {
    "objectID": "posts/epps-6356-prepare-for-class-5/data-visualization-and-data-science-embl/index.html",
    "href": "posts/epps-6356-prepare-for-class-5/data-visualization-and-data-science-embl/index.html",
    "title": "Data Visualization and Data Science (EMBL)",
    "section": "",
    "text": "He opens the video by talking about the conception of the tidyverse, including ggplot2, tibble, dplyr, and shiny, then introducing the audience to the famous animated visualization by Hans Rosling of countries and life expectancy.\nHe touches on ggplot2, and explains that he was inspired to create it after reading The Grammar of Graphics by Leland Wilkinson. The gg in ggplot2 stands for “grammar of graphics.” The way ggplot2 functions is by adding elements of a plot in layers, building an image from the ground up in an understandable way, instead of simply dictating a plot from the top down by typing the type of plot into the program.\nBut ggplot2 isn’t perfect. The standard plots it produces often use scientific notation that most people have trouble interpreting and the automatic color choices are often not colorblind friendly.\nA nice thing he said is that “the only way to get a really good visualization is to produce a bunch of not very good visualizations.” A nice comfort when I compare my novice nonsense to the works of those giants whom I now stand upon.\nOverall, I appreciated this insight into how ggplot2 works and how all the different packages of the tidyverse really fit together!"
  },
  {
    "objectID": "posts/epps-6356-prepare-for-class-5/the-parable-of-google-flu/index.html",
    "href": "posts/epps-6356-prepare-for-class-5/the-parable-of-google-flu/index.html",
    "title": "The Parable of Google Flu",
    "section": "",
    "text": "In a very rough summary, Google Flu Trends (hereby shortened to GFT) was Google’s attempt around fifteen years ago at predicting the number of doctor visits for influenza-like illness. The hope was that social media data could be used for this, in contrast to the more traditional data the CDC used to forecast the same thing. In fact, GFT was built specifically to predict the CDC’s reports.\nIt suffered from some common “Big Data” ills, like trying to use social media data to replace more traditionally collected data instead of merely supplementing it. The issue with this is the algorithm; Google’s algorithm is not stable, and is subject to change by Google’s engineers at any time. This constantly throws a spanner in the works, as each change to improve Google Search also changes the data generation process. On top of that, the 45 search terms used have never been documented (as of the time of the Nature article). This, to me, raises even more questions about repeatability and trust. Setting aside the question of how to reproduce research on an ever changing algorithm, it is hard for me to blindly accept the word of a corporation if they hold back the information needed to reproduce their findings, and leaving something as serious as public health in the hands of a secretive company strikes me as unwise.\nThe other glaring issues are overfitting and overparameterization. The way the Google team originally structured GFT was finding the best matches among 50 million search terms that correlated with 1152 data points. The chances of finding terms that correlated strongly with the previous data but did not have a strong ability to predict the flu were pretty high. As such, the original GFT was strongly seasonal and missed the nonseasonal “swine flu” pandemic of 2009. After this, Google updated the GFT algorithm, but it continued to significantly over-predict flu cases compared to the CDC for a few years.\nThe parable here is: just because you have an amount of data so vast that the human mind can barely comprehend it, doesn’t mean you are right. You still have to think about how the variables you select, and how you selected them, will affect the outcome of your research. Sometimes a small data perspective is needed."
  },
  {
    "objectID": "posts/epps-6356-prepare-for-class-6/comparing-data-visualizations/index.html",
    "href": "posts/epps-6356-prepare-for-class-6/comparing-data-visualizations/index.html",
    "title": "Comparing Different Styles of Data Visualization",
    "section": "",
    "text": "Today’s post is about comparing two different styles of data visualizations from two different authors."
  },
  {
    "objectID": "posts/epps-6356-prepare-for-class-6/thomas-lind-pederson/index.html",
    "href": "posts/epps-6356-prepare-for-class-6/thomas-lind-pederson/index.html",
    "title": "Reviewing the Work of Thomas Lind Pederson",
    "section": "",
    "text": "Today’s post is about reviewing the work of Thomas Lind Pederson, some of which can be found here."
  },
  {
    "objectID": "posts/epps-6356-prepare-for-class-7/literate-programming/index.html",
    "href": "posts/epps-6356-prepare-for-class-7/literate-programming/index.html",
    "title": "An Overview of Literate Programming",
    "section": "",
    "text": "Today’s post is an overview of the concept of “literate programming” by Knuth (1984)."
  },
  {
    "objectID": "posts/epps-6356-prepare-for-class-7/music-visualization/index.html",
    "href": "posts/epps-6356-prepare-for-class-7/music-visualization/index.html",
    "title": "The Elements of Music Visualization",
    "section": "",
    "text": "Today’s post is about the elements of music visualization, and I will be basing my discussion on this video “https://www.youtube.com/watch?v=dFDx-L7PcrY)”) by Stephen Malinowsky."
  },
  {
    "objectID": "posts/second-hackathon-results/index.html",
    "href": "posts/second-hackathon-results/index.html",
    "title": "Second Hackathon Results",
    "section": "",
    "text": "This is the result of the Hackathon for our team! This is a Shiny web application that displays three different graphs depending on a user input: either a Vehicles, Diamonds, or Economics graph.\nhttps://personmcdudeguy.shinyapps.io/assignment07/\nThe graphs are the same as my other assignment, but that’s fine by me!"
  },
  {
    "objectID": "posts/more-great-charts/index.html",
    "href": "posts/more-great-charts/index.html",
    "title": "More Great Charts!",
    "section": "",
    "text": "First up is the Bar Charts, shown here using the mpg dataset built into ggplot2.\n\n\n\nNext up is a bar chart of diamond prices by cut and color, using the diamonds dataset from ggplot.\n\n\n\nAnd lastly, a circular chart of unemployed Americans in the year 1968, using the economics dataset:\n\nThe code is below:\nlibrary(tidyverse)\n## Multiple Bar Chart\nggplot(data = mpg, aes(y = manufacturer)) +\ngeom_bar(fill = \"slategray3\") +\nfacet_wrap(vars(drv),\nlabeller = as_labeller(c(\"4\"= \"Four-Wheel Drive\",\nf= \"Front-Wheel Drive\",\nr= \"Rear-Wheel Drive\"))) +\nlabs(x = \"Count\", y = \"Make\", title = \"Vehicles by Drivetrain\") +\ntheme_bw() +\ntheme(\nstrip.background = element_rect(\ncolor = \"midnightblue\", fill = \"azure\", linetype=\"solid\")\n)\n## Multiple Column Chart\nggplot(data = diamonds, aes(x = cut, y = price, fill = color,\n)) +\ncoord_cartesian(ylim = c(15000, 19000)) +\ngeom_col(position = \"dodge\") +\nlabs(x = \"Cut\", y = \"Price\",\ntitle = \"Diamond Prices by Cut and Color\", fill = \"Colors\") +\ntheme_bw()\n## Circular Area Chart\n## can be created with a line chart graphed to polar coordinates\nss <- subset(economics, date>as.Date(\"1967-12-30\")&date<as.Date(\"1969-1-1\"))\nggplot(data = ss, aes(x = date, y = unemploy)) +\ngeom_area(fill = \"violet\", alpha = 0.5) +\ncoord_polar() +\nlabs(x = \"Date\", y = \"Unemployed (thousands)\",\ntitle = \"Unemployed Population in 1968\") +\ntheme_bw()"
  }
]